{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "    Compare the prediction accuracy of different models on the boston dataset\n",
      "================================================================================\n",
      "    \n",
      "OBJ: 4700.0251736\n",
      "OBJ: 3625.93508533\n",
      "OBJ: 3266.43201948\n",
      "OBJ: 3034.25307278\n",
      "OBJ: 2735.16436784\n",
      "OBJ: 2562.97351374\n",
      "OBJ: 2466.07974999\n",
      "OBJ: 2450.93706198\n",
      "OBJ: 2414.2867088\n",
      "OBJ: 2189.86457523\n",
      "OBJ: 2059.03360438\n",
      "OBJ: 2039.50375691\n",
      "OBJ: 1996.16738251\n",
      "OBJ: 1983.43477638\n",
      "OBJ: 1781.38177586\n",
      "OBJ: 1648.31438002\n",
      "OBJ: 1614.31969727\n",
      "OBJ: 1471.77144911\n",
      "OBJ: 1454.23874484\n",
      "OBJ: 1420.4026701\n",
      "OBJ: 1408.58236997\n",
      "OBJ: 1404.57690509\n",
      "OBJ: 1399.63395556\n",
      "OBJ: 1398.79973345\n",
      "OBJ: 1398.65156169\n",
      "OBJ: 1398.41246817\n",
      "OBJ: 1396.58468335\n",
      "OBJ: 1383.65876154\n",
      "OBJ: 1369.75460486\n",
      "OBJ: 1341.71183558\n",
      "OBJ: 1332.04763565\n",
      "OBJ: 1330.48592462\n",
      "OBJ: 1330.23912804\n",
      "OBJ: 1330.16679636\n",
      "OBJ: 1330.13900491\n",
      "OBJ: 1330.11883417\n",
      "OBJ: 1330.1176595\n",
      "OBJ: 1330.11706398\n",
      "OBJ: 1330.11638599\n",
      "OBJ: 1330.11634609\n",
      "OBJ: 1330.11628381\n",
      "OBJ: 1330.11620879\n",
      "OBJ: 1330.11602709\n",
      "OBJ: 1330.11598564\n",
      "OBJ: 1330.11577151\n",
      "OBJ: 1330.11415173\n",
      "OBJ: 1330.11383246\n",
      "OBJ: 1330.11377617\n",
      "OBJ: 1330.1137444\n",
      "OBJ: 1330.11371837\n",
      "OBJ: 1330.11368939\n",
      "OBJ: 1330.11368581\n",
      "OBJ: 1330.11368432\n",
      "OBJ: 1330.11368385\n",
      "OBJ: 1330.11368375\n",
      "OBJ: 1330.11368363\n",
      "OBJ: 1330.1136836\n",
      "OBJ: 1330.11368359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:262: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:262: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:262: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\scipy\\optimize\\_minimize.py:389: RuntimeWarning: Method TNC does not use Hessian-vector product information (hessp).\n",
      "  'information (hessp).' % method, RuntimeWarning)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:219: OptimizeWarning: Unknown solver options: maxfun\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cdc507eead35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    269\u001b[0m                                         solver='TNC')\n\u001b[0;32m    270\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mordinal_logistic_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ERROR (ORDINAL)  fold %s: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of ordered logistic regression (aka proportional odds) model\n",
    "https://en.wikipedia.org/wiki/Ordered_logit\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import linalg, optimize, sparse\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "BIG = 1e10\n",
    "SMALL = 1e-12\n",
    "\n",
    "\n",
    "def phi(t):\n",
    "    \"\"\"\n",
    "    logistic function, returns 1 / (1 + exp(-t))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size, dtype=np.float)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def log_logistic(t):\n",
    "    \"\"\"\n",
    "    (minus) logistic loss function, returns log(1 / (1 + exp(-t)))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = np.log(1 + np.exp(-t[idx]))\n",
    "    out[~idx] = (-t[~idx] + np.log(1 + np.exp(t[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ordinal_logistic_fit(X, y, alpha=0, l1_ratio=0, n_class=None, max_iter=10000,\n",
    "                         verbose=False, solver='TNC', w0=None):\n",
    "    \"\"\"\n",
    "    Ordinal logistic regression or proportional odds model.\n",
    "    Uses scipy's optimize.fmin_slsqp solver.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array, sparse matrix}, shape (n_samples, n_feaures)\n",
    "        Input data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    verbose: bool\n",
    "        Print convergence information\n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (n_features,)\n",
    "        coefficients of the linear model\n",
    "    theta : array, shape (k,), where k is the different values of y\n",
    "        vector of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    w0 = None\n",
    "\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError('Wrong shape for X and y')\n",
    "\n",
    "    # .. order input ..\n",
    "    idx = np.argsort(y)\n",
    "    idx_inv = np.zeros_like(idx)\n",
    "    idx_inv[idx] = np.arange(idx.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx].astype(np.int)\n",
    "    # make them continuous and start at zero\n",
    "    unique_y = np.unique(y)\n",
    "    for i, u in enumerate(unique_y):\n",
    "        y[y == u] = i\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # .. utility arrays used in f_grad ..\n",
    "    alpha = 0.\n",
    "    k1 = np.sum(y == unique_y[0])\n",
    "    E0 = (y[:, np.newaxis] == np.unique(y)).astype(np.int)\n",
    "    E1 = np.roll(E0, -1, axis=-1)\n",
    "    E1[:, -1] = 0.\n",
    "    E0, E1 = map(sparse.csr_matrix, (E0.T, E1.T))\n",
    "\n",
    "    def f_obj(x0, X, y):\n",
    "        \"\"\"\n",
    "        Objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        if np.any(c > 0):\n",
    "            return BIG\n",
    "\n",
    "        #loss = -(c[idx] + np.log(np.exp(-c[idx]) - 1)).sum()\n",
    "        loss = -np.log(1 - np.exp(c)).sum()\n",
    "\n",
    "        loss += b.sum() + log_logistic(b).sum() \\\n",
    "            + log_logistic(a).sum() \\\n",
    "            + .5 * alpha * w.dot(w) - np.log(z).sum()  # penalty\n",
    "        if np.isnan(loss):\n",
    "            pass\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "        return loss\n",
    "\n",
    "    def f_grad(x0, X, y):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        # gradient for w\n",
    "        phi_a = phi(a)\n",
    "        phi_b = phi(b)\n",
    "        grad_w = -X[k1:].T.dot(phi_b) + X.T.dot(1 - phi_a) + alpha * w\n",
    "\n",
    "        # gradient for theta\n",
    "        idx = c > 0\n",
    "        tmp = np.empty_like(c)\n",
    "        tmp[idx] = 1. / (np.exp(-c[idx]) - 1)\n",
    "        tmp[~idx] = np.exp(c[~idx]) / (1 - np.exp(c[~idx])) # should not need\n",
    "        grad_theta = (E1 - E0)[:, k1:].dot(tmp) \\\n",
    "            + E0[:, k1:].dot(phi_b) - E0.dot(1 - phi_a)\n",
    "\n",
    "        grad_theta[:-1] += 1. / np.diff(theta_0)\n",
    "        grad_theta[1:] -= 1. / np.diff(theta_0)\n",
    "        out = np.concatenate((grad_w, grad_theta))\n",
    "        return out\n",
    "\n",
    "    def f_hess(x0, s, X, y):\n",
    "        x0 = np.asarray(x0)\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        D = np.diag(phi(a) * (1 - phi(a)))\n",
    "        D_= np.diag(phi(b) * (1 - phi(b)))\n",
    "        D1 = np.diag(np.exp(-c) / (np.exp(-c) - 1) ** 2)\n",
    "        Ex = (E1 - E0)[:, k1:].toarray()\n",
    "        Ex0 = E0.toarray()\n",
    "        H_A = X[k1:].T.dot(D_).dot(X[k1:]) + X.T.dot(D).dot(X)\n",
    "        H_C = - X[k1:].T.dot(D_).dot(E0[:, k1:].T.toarray()) \\\n",
    "            - X.T.dot(D).dot(E0.T.toarray())\n",
    "        H_B = Ex.dot(D1).dot(Ex.T) + Ex0[:, k1:].dot(D_).dot(Ex0[:, k1:].T) \\\n",
    "            - Ex0.dot(D).dot(Ex0.T)\n",
    "\n",
    "        p_w = H_A.shape[0]\n",
    "        tmp0 = H_A.dot(s[:p_w]) + H_C.dot(s[p_w:])\n",
    "        tmp1 = H_C.T.dot(s[:p_w]) + H_B.dot(s[p_w:])\n",
    "        return np.concatenate((tmp0, tmp1))\n",
    "\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        import pylab as pl\n",
    "        pl.matshow(H_B)\n",
    "        pl.colorbar()\n",
    "        pl.title('True')\n",
    "        import numdifftools as nd\n",
    "        Hess = nd.Hessian(lambda x: f_obj(x, X, y))\n",
    "        H = Hess(x0)\n",
    "        pl.matshow(H[H_A.shape[0]:, H_A.shape[0]:])\n",
    "        #pl.matshow()\n",
    "        pl.title('estimated')\n",
    "        pl.colorbar()\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "    def grad_hess(x0, X, y):\n",
    "        grad = f_grad(x0, X, y)\n",
    "        hess = lambda x: f_hess(x0, x, X, y)\n",
    "        return grad, hess\n",
    "\n",
    "    x0 = np.random.randn(X.shape[1] + unique_y.size) / X.shape[1]\n",
    "    if w0 is not None:\n",
    "        x0[:X.shape[1]] = w0\n",
    "    else:\n",
    "        x0[:X.shape[1]] = 0.\n",
    "    x0[X.shape[1]:] = np.sort(unique_y.size * np.random.rand(unique_y.size))\n",
    "\n",
    "    #print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y))\n",
    "    #print(f_grad(x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y) - f_grad(x0, X, y))\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "\n",
    "    def callback(x0):\n",
    "        x0 = np.asarray(x0)\n",
    "        # print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "        if verbose:\n",
    "        # check that gradient is correctly computed\n",
    "            print('OBJ: %s' % f_obj(x0, X, y))\n",
    "\n",
    "    if solver == 'TRON':\n",
    "        import pytron\n",
    "        out = pytron.minimize(f_obj, grad_hess, x0, args=(X, y))\n",
    "    else:\n",
    "        options = {'maxiter' : max_iter, 'disp': 0, 'maxfun':10000}\n",
    "        out = optimize.minimize(f_obj, x0, args=(X, y), method=solver,\n",
    "            jac=f_grad, hessp=f_hess, options=options, callback=callback)\n",
    "\n",
    "    if not out.success:\n",
    "        warnings.warn(out.message)\n",
    "    w, theta = np.split(out.x, [X.shape[1]])\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def ordinal_logistic_predict(w, theta, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : coefficients obtained by ordinal_logistic\n",
    "    theta : thresholds\n",
    "    \"\"\"\n",
    "    unique_theta = np.sort(np.unique(theta))\n",
    "    out = X.dot(w)\n",
    "    unique_theta[-1] = np.inf # p(y <= max_level) = 1\n",
    "    tmp = out[:, None].repeat(unique_theta.size, axis=1)\n",
    "    return np.argmax(tmp < unique_theta, axis=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DOC = \"\"\"\n",
    "================================================================================\n",
    "    Compare the prediction accuracy of different models on the boston dataset\n",
    "================================================================================\n",
    "    \"\"\"\n",
    "    print(DOC)\n",
    "    from sklearn import cross_validation, datasets\n",
    "    boston = datasets.load_boston()\n",
    "    X, y = boston.data, np.round(boston.target)\n",
    "    #X -= X.mean()\n",
    "    y -= y.min()\n",
    "\n",
    "    idx = np.argsort(y)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    cv = cross_validation.ShuffleSplit(y.size, n_iter=50, test_size=.1, random_state=0)\n",
    "    score_logistic = []\n",
    "    score_ordinal_logistic = []\n",
    "    score_ridge = []\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        #test = train\n",
    "        if not np.all(np.unique(y[train]) == np.unique(y)):\n",
    "            # we need the train set to have all different classes\n",
    "            continue\n",
    "        assert np.all(np.unique(y[train]) == np.unique(y))\n",
    "        train = np.sort(train)\n",
    "        test = np.sort(test)\n",
    "        w, theta = ordinal_logistic_fit(X[train], y[train], verbose=True,\n",
    "                                        solver='TNC')\n",
    "        pred = ordinal_logistic_predict(w, theta, X[test])\n",
    "        1/0\n",
    "        s = metrics.mean_absolute_error(y[test], pred)\n",
    "        print('ERROR (ORDINAL)  fold %s: %s' % (i+1, s))\n",
    "        score_ordinal_logistic.append(s)\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        clf = linear_model.LogisticRegression(C=1.)\n",
    "        clf.fit(X[train], y[train])\n",
    "        pred = clf.predict(X[test])\n",
    "        s = metrics.mean_absolute_error(y[test], pred)\n",
    "        print('ERROR (LOGISTIC) fold %s: %s' % (i+1, s))\n",
    "        score_logistic.append(s)\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        clf = linear_model.Ridge(alpha=1.)\n",
    "        clf.fit(X[train], y[train])\n",
    "        pred = np.round(clf.predict(X[test]))\n",
    "        s = metrics.mean_absolute_error(y[test], pred)\n",
    "        print('ERROR (RIDGE)    fold %s: %s' % (i+1, s))\n",
    "        score_ridge.append(s)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('MEAN ABSOLUTE ERROR (ORDINAL LOGISTIC):    %s' % np.mean(score_ordinal_logistic))\n",
    "    print('MEAN ABSOLUTE ERROR (LOGISTIC REGRESSION): %s' % np.mean(score_logistic))\n",
    "    print('MEAN ABSOLUTE ERROR (RIDGE REGRESSION):    %s' % np.mean(score_ridge))\n",
    "    # print('Chance level is at %s' % (1. / np.unique(y).size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
